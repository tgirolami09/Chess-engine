{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d949afce",
   "metadata": {},
   "source": [
    "# Chess FEN → Score + Move Model\n",
    "This notebook:\n",
    "\n",
    "- Loads a `text` file where each line is `FEN|score_cp|other_cp|uci_move`.\n",
    "\n",
    "- Splits it into four pandas columns.\n",
    "\n",
    "- Encodes the FEN to numeric features (piece planes + side-to-move + castling + en passant info).\n",
    "\n",
    "- Trains:\n",
    "\n",
    "  1) A regressor to predict the first centipawn score.\n",
    "\n",
    "  2) A classifier to predict the **exact** UCI move string.\n",
    "\n",
    "- Evaluates and saves artifacts (`joblib` files + processed CSV).\n",
    "\n",
    "\n",
    "> **Data path**: update `DATA_PATH` below if your file is different. Upload your data to `/mnt/data/`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a64ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-17 17:25:50.318372: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# === Setup ===\n",
    "PROCESSED_PICKLE = \"./fullDf.pkl\"\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# from typing import List, Tuple\n",
    "# from collections import defaultdict\n",
    "\n",
    "# Models\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "\n",
    "import joblib\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from keras import losses\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3c9667",
   "metadata": {},
   "source": [
    "## 1) Load & split raw lines into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ffb9bfce",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Data file not found at ./useful_chess_data.txt. Upload your file there or change DATA_PATH.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m DATA_PATH = \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m./useful_chess_data.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m os.path.exists(DATA_PATH), \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData file not found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Upload your file there or change DATA_PATH.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m maxLines = \u001b[38;5;28mint\u001b[39m(\u001b[32m10\u001b[39m**\u001b[32m2\u001b[39m * \u001b[32m5.5\u001b[39m)\n\u001b[32m      6\u001b[39m amount = \u001b[32m38\u001b[39m*\u001b[32m10\u001b[39m**\u001b[32m6\u001b[39m\n",
      "\u001b[31mAssertionError\u001b[39m: Data file not found at ./useful_chess_data.txt. Upload your file there or change DATA_PATH."
     ]
    }
   ],
   "source": [
    "DATA_PATH = r\"./useful_chess_data.txt\"\n",
    "assert os.path.exists(DATA_PATH), f\"Data file not found at {DATA_PATH}. Upload your file there or change DATA_PATH.\"\n",
    "\n",
    "\n",
    "maxLines = int(10**2 * 5.5)\n",
    "amount = 38*10**6\n",
    "idx = 0\n",
    "lines = []\n",
    "skipped = 0\n",
    "with open(DATA_PATH, \"r\") as f:\n",
    "    for line in tqdm(f,desc=\"Reading positions\",total=38548203):\n",
    "        line = line.strip()\n",
    "        if line and idx%(amount//maxLines)==0:\n",
    "            lines.append(line)\n",
    "                # if (len(lines)%100000==0):\n",
    "                #     print(f\"Read {len(lines)}\")\n",
    "            if (len(lines)>=maxLines):\n",
    "                print(f\"Only using {len(lines)} lines\")\n",
    "                break\n",
    "        idx+=1\n",
    "\n",
    "df = pd.DataFrame([ln.split(\"|\") for ln in lines], columns=[\"fen\", \"score_cp\", \"score2_cp\", \"uci\"])\n",
    "df[\"score_cp\"] = pd.to_numeric(df[\"score_cp\"], errors=\"coerce\")\n",
    "df[\"score2_cp\"] = pd.to_numeric(df[\"score2_cp\"], errors=\"coerce\")\n",
    "\n",
    "df.to_pickle(\"fullDf.pkl\",\"zip\")\n",
    "\n",
    "print(\"Parsed chess data (preview)\")\n",
    "print(df.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "756191f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(\"fullDf.pkl\",\"zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e94020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3qk3/8/8/8/8/8/3PPPP1/4K3 b - - 0 1\n",
      "Original size: 550000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Equalizing dataframe: 100%|\u001b[32m███████████████████████████████\u001b[0m| 550000/550000 [01:09<00:00, 7867.39it/s]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented size: 1100000\n"
     ]
    }
   ],
   "source": [
    "#Code to balance dataset by adding mirror of game but switching the colors\n",
    "def reverse_fen(fen):\n",
    "    board, turn, *rest = fen.split(\" \")\n",
    "    # 1. Swap piece colors\n",
    "    swapped = \"\".join(\n",
    "        c.lower() if c.isupper() else c.upper() if c.islower() else c\n",
    "        for c in board\n",
    "    )\n",
    "    # 2. Reverse ranks\n",
    "    ranks = swapped.split(\"/\")\n",
    "    reversed_board = \"/\".join(ranks[::-1])\n",
    "    # 3. Flip turn\n",
    "    new_turn = \"w\" if turn == \"b\" else \"b\"\n",
    "    # 4. Rebuild FEN\n",
    "    new_fen = \" \".join([reversed_board, new_turn] + rest)\n",
    "    # 5. Flip eval\n",
    "    return new_fen\n",
    "\n",
    "testFen = \"4k3/3pppp1/8/8/8/8/8/3QK3 w - - 0 1\"\n",
    "print(f'Reversing fen \"{testFen}\" to \"{reverse_fen(testFen)}\"')\n",
    "\n",
    "#Augment dataframe\n",
    "def augmentDf(df):\n",
    "    print(\"Original size:\", len(df))\n",
    "    augmented_rows = []\n",
    "    for _, row in tqdm(df.iterrows(),total=len(df),desc=\"Equalizing dataframe\",colour=\"green\",ncols=100):\n",
    "        newFen = reverse_fen(row[\"fen\"])\n",
    "        augmented_rows.append({\n",
    "            \"fen\" : newFen,\n",
    "            \"score_cp\" : row[\"score_cp\"],\n",
    "            \"score2_cp\" : row[\"score2_cp\"],\n",
    "            \"uci\" : \"a1a1\",\n",
    "        })\n",
    "    df_aug = pd.DataFrame(augmented_rows)\n",
    "    print(\"Augmented size:\", len(df_aug)+len(df))\n",
    "    return pd.concat([df, df_aug], ignore_index=True)\n",
    "\n",
    "df = augmentDf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c94b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search df for black positions with high centipawn evaluations\n",
    "def searchDf(df):\n",
    "    blackRows = []\n",
    "    for _, row in tqdm(df.iterrows(),total=len(df),desc=\"Searching dataframe\",colour=\"green\",ncols=100):\n",
    "        if row[\"fen\"].split()[1] == \"b\" and row[\"uci\"] != \"a1a1\" and abs(row[\"score_cp\"]) >= 400:\n",
    "\n",
    "            blackRows.append({\n",
    "                \"fen\" : row[\"fen\"],\n",
    "                \"score_cp\" : row[\"score_cp\"],\n",
    "                \"score2_cp\" : row[\"score2_cp\"],\n",
    "                \"uci\" : row[\"uci\"],\n",
    "            })\n",
    "    print(len(blackRows))\n",
    "    print(*blackRows,sep=\"\\n\")\n",
    "\n",
    "searchDf(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f594c1d2",
   "metadata": {},
   "source": [
    "## 2) FEN encoder\n",
    "We encode each board into a feature vector: 64 squares × 12 piece planes (P,N,B,R,Q,K for white/black), plus side-to-move, castling rights (KQkq), and en passant file.\n",
    "\n",
    "We can also encode the FEN as an \"Image\" with 12 channels and add a couple of extra bits of information afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "12e9a3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature length of vector: 782\n",
      "Feature length of image (8, 8, 12) and (13,)\n",
      "Read 1 file_idx is now 0\n",
      "Read 1 file_idx is now 2\n",
      "Read 1 file_idx is now 4\n",
      "Read 2 file_idx is now 6\n",
      "Read 1 file_idx is now 0\n",
      "Read 1 file_idx is now 2\n",
      "Read 1 file_idx is now 4\n",
      "Read 2 file_idx is now 6\n",
      "Read 7 file_idx is now 0\n",
      "Read 5 file_idx is now 1\n",
      "Read 1 file_idx is now 7\n",
      "Read 1 file_idx is now 1\n",
      "Read 3 file_idx is now 3\n",
      "Read 1 file_idx is now 7\n",
      "Read 7 file_idx is now 0\n",
      "Read 1 file_idx is now 0\n",
      "Read 1 file_idx is now 2\n",
      "Read 4 file_idx is now 4\n",
      "Read 8 file_idx is now 0\n",
      "Feature length of flat (769,)\n"
     ]
    }
   ],
   "source": [
    "PIECE_TO_PLANE = {\n",
    "    'P':0,'N':1,'B':2,'R':3,'Q':4,'K':5,\n",
    "    'p':6,'n':7,'b':8,'r':9,'q':10,'k':11\n",
    "}\n",
    "\n",
    "def fen_to_feature_vector(fen: str) -> np.ndarray:\n",
    "    # FEN: <board> <side> <castling> <enpassant> <halfmove> <fullmove>\n",
    "    parts = fen.strip().split()\n",
    "    assert len(parts) >= 4, f\"Bad FEN: {fen}\"\n",
    "    board, side, castling, ep = parts[0], parts[1], parts[2], parts[3]\n",
    "\n",
    "    # Board: 8 ranks separated by '/'; each rank has pieces or digits for empty squares\n",
    "    planes = np.zeros((12, 8, 8), dtype=np.float32)\n",
    "    ranks = board.split('/')\n",
    "    assert len(ranks) == 8, f\"Bad board: {board}\"\n",
    "    for r, rank in enumerate(ranks):\n",
    "        file_idx = 0\n",
    "        for ch in rank:\n",
    "            if ch.isdigit():\n",
    "                file_idx += int(ch)\n",
    "            else:\n",
    "                if ch in PIECE_TO_PLANE:\n",
    "                    plane = PIECE_TO_PLANE[ch]\n",
    "                    planes[plane, r, file_idx] = 1.0\n",
    "                    file_idx += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown piece char: {ch}\")\n",
    "\n",
    "    features = []\n",
    "    features.extend(planes.reshape(-1).tolist())  # 12*8*8 = 768\n",
    "\n",
    "    # Side to move: 1 for white, 0 for black\n",
    "    features.append(1.0 if side == 'w' else 0.0)\n",
    "\n",
    "    # Castling rights: K, Q, k, q flags\n",
    "    features.append(1.0 if 'K' in castling else 0.0)\n",
    "    features.append(1.0 if 'Q' in castling else 0.0)\n",
    "    features.append(1.0 if 'k' in castling else 0.0)\n",
    "    features.append(1.0 if 'q' in castling else 0.0)\n",
    "\n",
    "    # En passant file (a-h → 0-7), or - if none\n",
    "    ep_file = -1\n",
    "    if ep != '-':\n",
    "        file_letter = ep[0]\n",
    "        if file_letter in \"abcdefgh\":\n",
    "            ep_file = \"abcdefgh\".index(file_letter)\n",
    "    # One-hot encode ep file into length 9 (none + 8 files)\n",
    "    for i in range(8):\n",
    "        features.append(1.0 if ep_file == i else 0.0)\n",
    "    features.append(1.0 if ep_file == -1 else 0.0)  # none flag\n",
    "\n",
    "    return np.array(features, dtype=np.float32)\n",
    "\n",
    "def fen_to_image_and_extra(fen: str):\n",
    "    parts = fen.strip().split()\n",
    "    board, side, castling, ep = parts[0], parts[1], parts[2], parts[3]\n",
    "\n",
    "    img = np.zeros((8,8,12), dtype=np.float32)\n",
    "    ranks = board.split('/')\n",
    "    for r, rank in enumerate(ranks):\n",
    "        file_idx = 0\n",
    "        for ch in rank:\n",
    "            if ch.isdigit():\n",
    "                file_idx += int(ch)\n",
    "            else:\n",
    "                img[r, file_idx, PIECE_TO_PLANE[ch]] = 1.0\n",
    "                file_idx += 1\n",
    "\n",
    "    # Extra bits\n",
    "    extras = []\n",
    "    extras.append(1.0 if side == 'w' else 0.0)\n",
    "    extras.append(1.0 if 'K' in castling else 0.0)\n",
    "    extras.append(1.0 if 'Q' in castling else 0.0)\n",
    "    extras.append(1.0 if 'k' in castling else 0.0)\n",
    "    extras.append(1.0 if 'q' in castling else 0.0)\n",
    "\n",
    "    ep_bits = [0.0]*8\n",
    "    if ep != '-' and ep[0] in \"abcdefgh\":\n",
    "        ep_bits[\"abcdefgh\".index(ep[0])] = 1.0\n",
    "    extras.extend(ep_bits)\n",
    "\n",
    "    return img, np.array(extras, dtype=np.float32)\n",
    "\n",
    "def fen_to_flat(fen: str, debug=False):\n",
    "    parts = fen.strip().split()\n",
    "    board, side, castling, ep = parts[0], parts[1], parts[2], parts[3]\n",
    "    img = np.zeros((8,8,12), dtype=np.float16)\n",
    "    ranks = board.split('/')\n",
    "    for r, rank in enumerate(ranks):\n",
    "        file_idx = 0\n",
    "        for ch in rank:\n",
    "            if ch.isdigit():\n",
    "                print(f\"Read {ch} file_idx is now {file_idx}\")\n",
    "                file_idx += int(ch)\n",
    "            else:\n",
    "                if (PIECE_TO_PLANE[ch] == 10):\n",
    "                    print(f\"Piece {ch} is on position {r,file_idx}\")\n",
    "                img[r, file_idx, PIECE_TO_PLANE[ch]] = 1.0\n",
    "                file_idx += 1\n",
    "\n",
    "    if (debug):\n",
    "        print(\"8 x 8 x 12, without extras\")\n",
    "        print(*img)\n",
    "\n",
    "    features = []\n",
    "    features.extend(img.reshape(-1).tolist())\n",
    "\n",
    "    features.append(1.0 if side == 'w' else 0.0)\n",
    "\n",
    "    # return img.flatten()\n",
    "    return np.array(features, dtype=np.float16)\n",
    "\n",
    "# Quick sanity check\n",
    "x = fen_to_feature_vector(df.iloc[0]['fen'])\n",
    "print('Feature length of vector:', x.shape[0])\n",
    "\n",
    "x = fen_to_image_and_extra(df.iloc[0][\"fen\"])\n",
    "print(f\"Feature length of image {x[0].shape} and {x[1].shape}\")\n",
    "\n",
    "x = fen_to_flat(df.iloc[0][\"fen\"])\n",
    "print(f\"Feature length of flat {x.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee20f241",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Little test to understand how flatening works in np\n",
    "test_np = np.zeros((8,8,12), dtype=np.float32)\n",
    "for i in range(8):\n",
    "    for j in range(8):\n",
    "        for k in range(12):\n",
    "            test_np[i,j,k] = i*10000+j*100+k\n",
    "\n",
    "test_features = []\n",
    "test_features.extend(test_np.reshape(-1).tolist())\n",
    "for idx,val in enumerate(test_features):\n",
    "    print(f\"Idx:{idx} val:{val}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861f3f11",
   "metadata": {},
   "source": [
    "## 3) Build feature matrix X and target y\n",
    "- `y_score` = first centipawn score (`score_cp`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bfe627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (200000, 782)\n",
      "Sample y_score: [ -34. -109.  -33.  -33.  -89.]\n"
     ]
    }
   ],
   "source": [
    "# Build X features\n",
    "X = np.stack(df['fen'].apply(fen_to_feature_vector).values)\n",
    "\n",
    "\n",
    "print('X shape:', X.shape)\n",
    "\n",
    "# Train/validation split\n",
    "X_train, X_test = train_test_split(\n",
    "    X, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8329066e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_score = df['score_cp'].values.astype(np.float16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6a0405",
   "metadata": {},
   "source": [
    "### Prepare data for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "590e13cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Formating NN training and validation data: 100%|\u001b[32m██████████\u001b[0m| 550000/550000 [00:55<00:00, 9891.88it/s] \n"
     ]
    }
   ],
   "source": [
    "board_imgs = []\n",
    "for fen,static_score in tqdm(zip(df[\"fen\"],df[\"score2_cp\"]),total=len(df),desc=\"Formating NN training and validation data\",colour=\"green\"):\n",
    "    img = fen_to_flat(fen)\n",
    "    img = np.append(img, np.float16(static_score))\n",
    "    board_imgs.append(img)\n",
    "\n",
    "board_imgs = np.array(board_imgs)\n",
    "\n",
    "imgs_train, imgs_test, y_score_train, y_score_test = train_test_split(\n",
    "    board_imgs, y_score, test_size=0.2, random_state=42\n",
    ")\n",
    "y_score_train = y_score_train.reshape(-1, 1).astype(\"float16\")\n",
    "y_score_test  = y_score_test.reshape(-1, 1).astype(\"float16\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e40a28b",
   "metadata": {},
   "source": [
    "## 4) Models\n",
    "- **Score regressor**: Ridge regression\n",
    "- **Score preditor**: NN with tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de7b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_chess_cnn():\n",
    "    # board_input = Input(shape=(8,8,12), name=\"board\")\n",
    "    # x = layers.Conv2D(16, (3,3), activation=\"relu\", padding=\"same\")(board_input)\n",
    "    # x = layers.Dropout(0.25)(x)\n",
    "    # x = layers.Conv2D(16, (2,2), activation=\"relu\", padding=\"same\")(x)\n",
    "    # x = layers.Dropout(0.25)(x)\n",
    "    # x = layers.Flatten()(x)\n",
    "\n",
    "\n",
    "    # extra_input = Input(shape=(n_extra,), name=\"extra\")\n",
    "    # merged = layers.concatenate([x, extra_input])\n",
    "    # merged = layers.Dense(16, activation=\"relu\")(board_input)\n",
    "    # merged = layers.Dropout(0.5)(merged)\n",
    "\n",
    "    board_input = Input(shape=(770,), name=\"board\")\n",
    "    # hidden = layers.Dense(32, activation=\"relu\",kernel_regularizer=l2(0.001))(board_input)\n",
    "    # hidden = layers.Dropout(0.2)(hidden)\n",
    "    hidden = layers.Dense(32, activation=\"relu\",kernel_regularizer=l2(0.001))(board_input)\n",
    "    for i in range(7):\n",
    "        hidden = layers.Dense(16, activation=\"relu\",kernel_regularizer=l2(0.001))(hidden)\n",
    "    # hidden = layers.Dropout(0.20)(hidden)\n",
    "    value_out = layers.Dense(1, name=\"value\")(hidden)\n",
    "\n",
    "\n",
    "    # model = models.Model(inputs=[board_input, extra_input], outputs=[value_out])\n",
    "    model = models.Model(inputs=[board_input], outputs=[value_out])\n",
    "    model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss={\"value\": \"mse\"},\n",
    "    # loss = losses.Huber(delta=5.0), \n",
    "    metrics={\"value\": \"mae\"}\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "#A NN just ot see if it works\n",
    "model = build_chess_cnn()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7309c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(imgs_train[0][0]))\n",
    "print(type(y_score_train[0]))\n",
    "\n",
    "print(imgs_train.shape, imgs_train.dtype)\n",
    "print(imgs_test.shape, imgs_test.dtype)\n",
    "print(y_score_train.shape, y_score_train.dtype)\n",
    "print(y_score_test.shape, y_score_test.dtype)\n",
    "\n",
    "print(\"Any NaNs in imgs_train?\", np.isnan(imgs_train).any())\n",
    "print(\"Any NaNs in y_score_train?\", np.isnan(y_score_train).any())\n",
    "print(\"Any strings in y_score_train?\", any(isinstance(x, str) for x in y_score_train))\n",
    "print(\"Any strings in board_imgs?\", any(isinstance(x, str) for x in board_imgs))\n",
    "print(\"Any strings in y_score?\", any(isinstance(x, str) for x in y_score))\n",
    "\n",
    "print(board_imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c4b79c19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 38ms/step - loss: 866.8355 - mae: 22.6789 - val_loss: 850.5442 - val_mae: 22.4630 - learning_rate: 2.4414e-07\n",
      "Epoch 2/100\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 852.4680 - mae: 22.4791 - val_loss: 841.2231 - val_mae: 22.3331 - learning_rate: 2.4414e-07\n",
      "Epoch 3/100\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 842.6504 - mae: 22.3363 - val_loss: 835.0042 - val_mae: 22.2456 - learning_rate: 2.4414e-07\n",
      "Epoch 4/100\n",
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 19ms/step - loss: 838.1335 - mae: 22.2499 - val_loss: 830.0657 - val_mae: 22.1740 - learning_rate: 2.4414e-07\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m reduce_lr = ReduceLROnPlateau(monitor = \u001b[33m\"\u001b[39m\u001b[33mval_loss\u001b[39m\u001b[33m\"\u001b[39m, mode = \u001b[33m\"\u001b[39m\u001b[33mmin\u001b[39m\u001b[33m\"\u001b[39m, factor = \u001b[32m0.5\u001b[39m, patience = \u001b[32m3\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#Train CNN model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_score_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# validation_data=({\"board\": imgs_test, \"extra\": extras_test}, {\"value\": y_score_test),\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_score_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2048\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mearly_stop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduce_lr\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/keras/src/backend/tensorflow/trainer.py:370\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator.catch_stop_iteration():\n\u001b[32m    369\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m--> \u001b[39m\u001b[32m370\u001b[39m         \u001b[43mcallbacks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    371\u001b[39m         logs = \u001b[38;5;28mself\u001b[39m.train_function(iterator)\n\u001b[32m    372\u001b[39m         callbacks.on_train_batch_end(step, logs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/Python/3.11/lib/python/site-packages/keras/src/callbacks/callback_list.py:147\u001b[39m, in \u001b[36mCallbackList.on_train_batch_begin\u001b[39m\u001b[34m(self, batch, logs)\u001b[39m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n\u001b[32m    145\u001b[39m         callback.on_epoch_end(epoch, logs)\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    148\u001b[39m     logs = python_utils.pythonify_logs(logs)\n\u001b[32m    149\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.callbacks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#Arreter en cas d'overfitting ou de stagnation\n",
    "early_stop = EarlyStopping(monitor = \"val_loss\", patience = 5, restore_best_weights=True)\n",
    "\n",
    "#Permettre qu'il continue meme si plus lentement\n",
    "reduce_lr = ReduceLROnPlateau(monitor = \"val_loss\", mode = \"min\", factor = 0.5, patience = 3)\n",
    "\n",
    "#Train CNN model\n",
    "history = model.fit(\n",
    "    imgs_train,\n",
    "    y_score_train,\n",
    "    # validation_data=({\"board\": imgs_test, \"extra\": extras_test}, {\"value\": y_score_test),\n",
    "    validation_data=(imgs_test,y_score_test),\n",
    "    epochs=100,\n",
    "    batch_size=2048,\n",
    "    verbose=1,\n",
    "    callbacks = [early_stop, reduce_lr]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fa8c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# plt.plot(history.history['loss'], label=\"Train loss\")\n",
    "# plt.plot(history.history['val_loss'], label=\"Validation loss\")\n",
    "plt.plot(history.history[\"mae\"], label=\"MAE\")\n",
    "plt.plot(history.history[\"val_mae\"], label=\"Validation MAE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7eb7a822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m215/215\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 894.3179 - mae: 23.0600\n",
      "Test results: [892.817626953125, 23.034631729125977]\n"
     ]
    }
   ],
   "source": [
    "#Test CNN model\n",
    "test_results = model.evaluate(\n",
    "    # {\"board\": imgs_test, \"extra\": extras_test},\n",
    "    imgs_test,\n",
    "    y_score_test,\n",
    "    batch_size=1024,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#Best result\n",
    "# [696.7526245117188, 19.979074478149414]\n",
    "#Best on doubled\n",
    "# [754.0731201171875, 21.048927307128906]\n",
    "# [833.1061401367188, 22.360122680664062]\n",
    "print(\"Test results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1fef96e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 2 file_idx is now 1\n",
      "Piece q is on position (0, 3)\n",
      "Read 4 file_idx is now 5\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,2.39\n",
      "(1, 770)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
      "Predicted evaluation (centipawns) for 'r2qk4/8/8/8/8/8/PPPPPPPP/RNBQKBNR w KQq - 0 1' : 13.414\n",
      "Piece q is on position (0, 3)\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "Read 8 file_idx is now 0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0\n",
      "(1, 770)\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
      "Predicted evaluation (centipawns) for 'rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1' : 5.829\n"
     ]
    }
   ],
   "source": [
    "def predict_eval(fen,static_eval):\n",
    "    x_input = fen_to_flat(fen,debug=False)\n",
    "    x_input = np.append(x_input, np.float32(static_eval))\n",
    "    print(*x_input,sep=\",\")\n",
    "    x_input = x_input.reshape(1, -1)\n",
    "\n",
    "    print(x_input.shape)\n",
    "    prediction = model.predict(x_input)\n",
    "    print(f\"Predicted evaluation (centipawns) for '{fen}' : {prediction[0][0]:.3f}\")\n",
    "\n",
    "predict_eval(\"r2qk4/8/8/8/8/8/PPPPPPPP/RNBQKBNR w KQq - 0 1\",2.390)\n",
    "predict_eval(\"rnbqkbnr/pppppppp/8/8/8/8/PPPPPPPP/RNBQKBNR w KQkq - 0 1\",0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af256dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "nb_tests = 100\n",
    "totalError = 0\n",
    "for i in range(nb_tests):\n",
    "    wantedIdx = random.randint(0,10**5)\n",
    "    x_input = fen_to_flat(df[\"fen\"][wantedIdx])\n",
    "    x_input = np.append(x_input, np.float32(df[\"score2_cp\"][wantedIdx]))\n",
    "    x_input = x_input.reshape(1, -1)\n",
    "\n",
    "    print(x_input.shape)\n",
    "    prediction = model.predict(x_input)\n",
    "    error = int(df['score_cp'][wantedIdx])-(prediction[0][0])\n",
    "    totalError+=abs(error)\n",
    "    # print(f\"Predicted evaluation (centipawns) for '{df['fen'][wantedIdx]}' : {prediction[0][0]:.3f} accurate is {df['score_cp'][wantedIdx]}, error is {error:.3f}\")\n",
    "print(f\"Mean absolute error = {totalError/nb_tests}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6440d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: test/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'test'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 770), dtype=tf.float32, name='board')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  5150825056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150826112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150825760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150826640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150826288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150827168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150826992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150827520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150827344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150827872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150827696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150828224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150828048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150828576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150828400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150828928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150828752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  5150829280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "# model.save(\"test.keras\")\n",
    "model.export(\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e478406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ board (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">770</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,672</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">528</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">272</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ value (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ board (\u001b[38;5;33mInputLayer\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m770\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_24 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m24,672\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_25 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m528\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_26 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_31 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m)             │           \u001b[38;5;34m272\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ value (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m17\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">80,549</span> (314.65 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m80,549\u001b[0m (314.65 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,849</span> (104.88 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,849\u001b[0m (104.88 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">53,700</span> (209.77 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m53,700\u001b[0m (209.77 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(\"best_on_doubled.keras\")  \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f890b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regressor for score\n",
    "regressor = Ridge(alpha=1, random_state=42)\n",
    "regressor.fit(X_train, y_score_train)\n",
    "print(f\"Fitted model for score\")\n",
    "\n",
    "# Evaluate\n",
    "y_score_pred = regressor.predict(X_test)\n",
    "mae = mean_absolute_error(y_score_test, y_score_pred)\n",
    "r2 = r2_score(y_score_test, y_score_pred)\n",
    "\n",
    "#Best 27.44\n",
    "print(f\"Score MAE (cp): {mae:.2f}\")\n",
    "#Best 0.909\n",
    "print(f\"Score R^2: {r2:.3f}\")\n",
    "\n",
    "# Save artifacts\n",
    "joblib.dump(regressor, REGRESSOR_PATH)\n",
    "print(\"Saved regressor to disk.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384ea86d",
   "metadata": {},
   "source": [
    "## 5) Inference helper\n",
    "`predict_score_and_move(fen)` → `(score_cp_pred, uci_move_pred)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697b6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_score_and_move(fen: str) -> float:\n",
    "    x = fen_to_feature_vector(fen).reshape(1, -1)\n",
    "    score_pred = float(regressor.predict(x)[0])\n",
    "    return score_pred\n",
    "\n",
    "# Demo with first row\n",
    "demo_score = predict_score_and_move(df.iloc[0]['fen'])\n",
    "print('Demo prediction:', demo_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e021d4",
   "metadata": {},
   "source": [
    "## 6) Export: metrics + sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e612baf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Collect some sample predictions for inspection\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m n_show = \u001b[38;5;28mmin\u001b[39m(\u001b[32m10\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mX_test\u001b[49m))\n\u001b[32m      3\u001b[39m sample_rows = []\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_show):\n",
      "\u001b[31mNameError\u001b[39m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect some sample predictions for inspection\n",
    "n_show = min(10, len(X_test))\n",
    "sample_rows = []\n",
    "for i in range(n_show):\n",
    "    fen = df.iloc[i]['fen']\n",
    "    true_score = df.iloc[i]['score_cp']\n",
    "    true_move = df.iloc[i]['uci']\n",
    "    ps = predict_score_and_move(fen)\n",
    "    sample_rows.append({\n",
    "        \"fen\": fen,\n",
    "        \"true_score_cp\": true_score,\n",
    "        \"pred_score_cp\": round(ps, 2),\n",
    "    })\n",
    "samples_df = pd.DataFrame(sample_rows)\n",
    "samples_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
